---
title: "CEVE 543 Fall 2025 Lab 4: Bayesian Rainfall Analysis"
subtitle: "Iterative prior specification, Bayesian GEV workflow, Two-station comparison with ArviZ.jl"
author: CEVE 543 Fall 2025
date: "2025-09-19"
type: "lab"
module: 1
week: 4
objectives:
  - "Set up Bayesian GEV models with informative priors"
  - "Develop strategies for prior specification in extreme value analysis"
  - "Compare rainfall patterns between two stations using Bayesian inference"
ps_connection: "Builds Bayesian GEV toolkit for PS1 Tasks 1 and 3"

engine: julia
julia:
  exeflags: ["+1.11"]

format:
  html:
    toc: true
    toc-depth: 3
    code-block-bg: "#f8f8f8"
    code-block-border-left: "#e1e4e5"
    theme: simplex
    number-sections: true
    fig-format: svg
    code-annotations: hover
  typst:
    fontsize: 11pt
    margin:
      x: 1in
      y: 1in
    number-sections: true
    fig-format: svg
    echo: false
    code-annotations: false

execute:
  cache: false
  freeze: auto

# Code formatting options
code-overflow: wrap
code-line-numbers: false
code-block-font-size: "0.85em"
---

## Do Before Lab {.unnumbered}

1. **Accept the assignment.** Use the GitHub Classroom link posted on Canvas to "accept" the assignment, which will give you your own GitHub repository.
2. **Clone your repository** to your computer using GitHub Desktop or the command line.
3. **Open the repository in VS Code** directly from VS Code, GitHub Desktop, or your command line.
4. **Activate the Julia environment.** In VS Code, open the command palette (View → Command Palette) and type "Julia: Start REPL". Alternatively, run `julia --project=.` in the terminal, then run `using Pkg; Pkg.instantiate()` to install required packages. The easiest method is to type `]` in the REPL to enter package mode, then type `instantiate`.
5. **Review the Bayesian workflow concepts** from Module 1, particularly prior specification and posterior inference.
6. **Verify rendering works.** Run `quarto render index.qmd` in your terminal - it should create both PDF and HTML documents.

## Do During Lab {.unnumbered}

This lab guides you through implementing a complete Bayesian workflow for extreme value analysis:

1. **Develop iterative priors** using domain knowledge and prior predictive checks
2. **Implement Bayesian GEV inference** using Turing.jl with informative priors
3. **Compare two stations** using the same Bayesian framework
4. **Validate your models** using modern Bayesian diagnostics with ArviZ.jl

As with previous labs, you do not need to write code from scratch -- focus on understanding the workflow, modifying parameters, and interpreting results.

To submit your work, push your changes to GitHub, render to PDF, and upload the PDF to Canvas.

## Your Analysis {.unnumbered}


**Response 1: Prior Evolution and Justification**

The posterior confidence interval is definitely a big upgrade compared to the prior confidence interval in terms of narrowing it down.  The range of the distributions is about what I'd expect if we assume stationarity. 

**Response 2: Data Learning and Posterior Insights**

I definitely expected Galveston to have both a wider distribution, and also a higher end to that distribution, when compared to Alvin.  I also didn't expect the scale parameter to be the most different thing between the two of them.  

Ultimately, we live in an era where our data on these events is still limited to only about 100 years at most.  Domain knowledge is critical for us, since we haven't had enought time to establish robust trends solely based on empirical evidence, as shown by the Alvin vs. Galveston comparison being thrown off by one major storm. 

**Response 3: Two-Station Bayesian Comparison**

The priors should definitely not apply to both stations, or at least, the scale parameter shouldn't.  

In theory, Galveston should have more extreme weather due to its more coastal location, but in practice, Alvin has a more extreme distribution.  This is an example of how domain knowledge is critical for regional analysis, as our empirical observations run in contradiction to it. 

**Response 4: Uncertainty Quantification and Model Validation**

Bayesian uncertainty quantification is much better able to capture the range of possible values when compared to MLE.  Our diagnostics reflect a reliable model, with Rhat values of 1.00 for each factor.

**Response 5: Bayesian Workflow for PS1**

This will help us do section 4, the regional parameter estimation section.  It'll help us select which GEV values should vary or remain stationary across which regions. 

I anticipate that applying these principles across time to do non-stationary analysis will be the biggest challenge. 

## Data Setup and Package Loading

### Loading Required Packages

Instead of using the package manager to activate the environment each time, we can do it programmatically.
This approach ensures we're using the correct package versions and may help you avoid some common environment issues.

```{julia}
#| output: false
using Pkg
lab_dir = dirname(@__FILE__)  # <1>
Pkg.activate(lab_dir)
# Pkg.instantiate() # uncomment this the first time you run the lab to install packages, then comment it back
```

1. Get the directory path where this notebook file is located

We can load in some packages for Bayesian analysis.
See [Turing](https://turinglang.org/) and [ArviZ](https://arviz-devs.github.io/ArviZ.jl/dev/quickstart/) docs for more details.

```{julia}
#| output: false
using Turing
using ArviZ
using Distributions
using Random
using NCDatasets
using Optim
```

We use [Makie](https://docs.makie.org/) with the CairoMakie backend for high-quality plotting and set it to output images as SVG files for crisp rendering:

```{julia}
#| output: false
using CairoMakie
CairoMakie.activate!(type="svg")
```

We load some utility packages for data manipulation and downloading files.
See the [TidierData](https://tidierorg.github.io/TidierData.jl/latest/) docs for more details on the `@chain` macro syntax.

```{julia}
#| output: false
using Downloads
using DataFrames
ENV["DATAFRAMES_ROWS"] = 5
using TidierData
```

Next, we include some utility functions from previous labs.
Note that this file loads several additional packages that are not explicitly loaded here.

```{julia}
#| output: false
include("util.jl")
```

Finally, it's good practice to set a random seed for reproducible results across different runs:

```{julia}
#| output: false
rng = MersenneTwister(543)
```

### Loading Houston Precipitation Data

We'll use the same Texas precipitation dataset from Labs 2 and 3, using a [`let...end`](https://docs.julialang.org/en/v1/manual/variables-and-scoping/#Let-Blocks) block to encapsulate the download and reading logic so that variables don't leak into the global scope.

```{julia}
#| output: false
stations, rainfall_data = let  # <1>
    precip_fname = joinpath(lab_dir, "dur01d_ams_na14v11.txt")
    url = "https://hdsc.nws.noaa.gov/pub/hdsc/data/tx/dur01d_ams_na14v11.txt"

    if !isfile(precip_fname)
        Downloads.download(url, precip_fname)
    end
    read_noaa_data(precip_fname)
end
```

1. Use a `let...end` block for local scoping of temporary variables

`let...end` blocks keep variables scoped locally, so if you try to access `precip_fname` or `url` outside the block, you'll get an error.

Next, choose the same station you used in Lab 3 for your analysis.


```{julia}
my_stnid = 779  # REQUIRED: replace with your chosen station ID
my_station = @chain stations begin
    @filter(stnid == !!my_stnid)  # <1>
    first
end
my_rainfall = @chain rainfall_data begin
    @filter(stnid == !!my_stnid)
    @arrange(date)
end

station_time_series = let
    fig = Figure(size=(1000, 400))
    ax = Axis(fig[1, 1], xlabel="Year", ylabel="Annual Max Rainfall (inches)",
        title="Annual Maximum Rainfall at $(my_station.name), TX")
    years = my_rainfall.year
    rain = ustrip.(u"inch", my_rainfall.rainfall)  # <2>
    lines!(ax, years, rain, color=:blue)
    scatter!(ax, years, rain, color=:blue, markersize=8)
    fig
end
```

1. The !! escapes the variable for use inside the macro
2. Remove physical units for plotting (convert to plain numbers)

## Iterative Prior Development

**Before looking at any data**, think about what you know about extreme rainfall in Houston.
What's your frame of reference?
Are you thinking about:

- General knowledge of rainfall patterns anywhere?
- Houston's specific climate characteristics?
- Personal experience with storms?
- Engineering design standards?

As a starting point, I wrote down some initial very weak priors. 
However, they are likely not very realistic for Houston's climate.
We will iteratively refine them based on [prior predictive checks](https://arxiv.org/abs/2011.01808) and domain knowledge.

```{julia}
#| output: false
@model function gev_model(y)
    μ ~ Normal(3.0, 3.0)
    log_σ ~ Normal(0.0, 1.0)  # <1>
    ξ ~ Normal(0.0, 0.3)
    σ = exp(log_σ)  # <2>
    dist = GeneralizedExtremeValue(μ, σ, ξ)
    if length(y) > 0  # <3>
        for i in eachindex(y)
            y[i] ~ dist
        end
    end
end
```

1. Log-scale parameter prior (ensures σ > 0 after transformation)
2. Transform log-scale to positive scale parameter
3. Only fit to data if observations are provided (allows prior-only sampling)

Prior predictive checking asks us to simulate data from our prior distributions and see if the simulated data looks reasonable.
While the typical approach is to simulate $y_{\text{rep}} \sim p(y | \theta)$, we can also look at the return level curves, which we can compute directly from the parameters.

First, we need to draw samples from our prior distribution.
Turing helps us with this by allowing us to run MCMC with no data, which effectively samples from the prior only.

```{julia}
#| warning: false
#| output: false
function load_or_sample(fname, model; overwrite=false, n_chains=4, samples_per_chain=2000, sampler=NUTS(), threading=MCMCThreads(), rng=rng)
    idata = try
        @assert !overwrite "Reading from cache disabled by overwrite=true"  # <1>
        idata = ArviZ.from_netcdf(fname)
        @info "Loaded cached prior samples from $fname"
        return idata
    catch
        chains = sample(
            model,
            sampler,
            threading,
            Int(ceil(samples_per_chain * n_chains)),  # <2>
            n_chains, # number of chains
            verbose=false,
        )
        idata = ArviZ.from_mcmcchains(chains)
        ArviZ.to_netcdf(idata, fname)
        @info "Sampled and cached prior samples to $fname"
        return idata
    end
end
```

1. Check if overwrite flag allows loading from cache
2. Calculate total number of samples across all chains

```{julia}
#| output: false
prior_idata_v1 = let
    fname = joinpath(lab_dir, "prior_v1.nc")
    model = gev_model([])  # <1>
    overwrite = true # <2>
    load_or_sample(fname, model; overwrite=overwrite)
end
prior_GEVs_v1 = vec(GeneralizedExtremeValue.(prior_idata_v1.posterior.μ, exp.(prior_idata_v1.posterior.log_σ), prior_idata_v1.posterior.ξ))
```

1. Create model with empty data array (prior-only sampling)
2. Set `overwrite=true` to force re-sampling from the prior; set to `false` to load cached samples

We can see that this returns an object of type `InferenceData`. 
Click on the triangles to expand and see more.
(Annoyingly, even though we sampled from the prior only, ArviZ still calls it "posterior" unless we do a rather painful workaround Let's not get hung up on that.)
Extract the parameter samples and create the prior predictive visualization.
Each line represents a different possible return level curve from our prior beliefs.

```{julia}
let
    fig = Figure(size=(1200, 600))
    ax = Axis(fig[1, 1], xlabel="Return Period (years)", ylabel="Return Level (inches)",
        title="Prior v1 Predictive Distribution", xscale=log10, xticks=[1, 2, 5, 10, 25, 50, 100, 250])
    rts = logrange(1.1, 250, 500)
    for i in rand(1:length(prior_GEVs_v1), 250)
        gev = prior_GEVs_v1[i]
        add_return_level_curve!(ax, gev, rts; color=(:blue, 0.125))
    end
    ylims!(ax, 0, 75)
    fig
end
```

::: {.callout-note}
## Think

Are these return level curves reasonable for Houston's climate, based on your domain knowledge?

I would say yes!  Considering Harvey was about 30 inches of rainfall, and was considered in excess of the 500 year storm. 
:::

### Priors on Return Levels

The first iteration showed that parameter-based priors can produce unrealistic scenarios.
One of the key challenges is that we write a prior over the parameters themselves, but they are not in fact independent.
Instead of thinking about abstract GEV parameters ($\mu$, $\sigma$, $\xi$), let's specify priors on quantities we understand: **return levels**.

Return levels are much more intuitive for domain experts:

- "The 2-year flood should be around 5 inches"
- "The 100-year flood might be 15-20 inches"
- "A 500-year event could reach 25+ inches"

We encode these believes as a Julia data type, called a `struct`, and pass in the return period in years, the mean return level, and the standard deviation (uncertainty) around that mean.
Under the hood, it converts these to an Inverse Gamma distribution, which is lower-bounded at zero and has a long right tail, which is appropriate for rainfall amounts.

Start by defining your return level beliefs.

```{julia}
#| output: false
return_level_priors = [
    ReturnLevelPrior(2, 3.0, 1.5),
    ReturnLevelPrior(10, 8.0, 4),
    ReturnLevelPrior(50, 12.0, 6),
    ReturnLevelPrior(100, 16, 8),
]
```

We can pass these into our Turing model

```{julia}
#| output: false
@model function gev_model_quantile_priors(y; return_level_priors=[])

    μ ~ Normal(3.0, 3.0)
    log_σ ~ Normal(0.0, 1.0)
    ξ ~ Normal(0.0, 0.3)
    σ = exp(log_σ)
    dist = GeneralizedExtremeValue(μ, σ, ξ)

    # Apply return level constraints
    for prior in return_level_priors
        rl = quantile(dist, prior.quantile)  # <1>
        if rl > 0.1
            Turing.@addlogprob!(loglikelihood(prior.distribution, rl))  # <2>
        else
            Turing.@addlogprob!(-Inf)  # <3>
        end
    end

    # Data likelihood
    if length(y) > 0
        y .~ dist
    end
end
```

1. Calculate the return level for this prior's return period
2. Add log-probability of return level under the prior distribution
3. Assign impossible probability to negative return levels

As before, we generate these samples

```{julia}
#| output: false
prior_idata_v2 = let
    fname = joinpath(lab_dir, "prior_v2.nc")
    model = gev_model_quantile_priors([]; return_level_priors=return_level_priors)
    overwrite = false
    load_or_sample(fname, model; overwrite=overwrite)
end
prior_GEVs_v2 = vec(GeneralizedExtremeValue.(prior_idata_v2.posterior.μ, exp.(prior_idata_v2.posterior.log_σ), prior_idata_v2.posterior.ξ))
```

We can compare these two prior versions side by side

```{julia}
fig_prior_comparison = let
    rts = logrange(1.1, 250, 500)
    xticks = [1, 2, 5, 10, 25, 50, 100, 250]
    fig = Figure(size=(1200, 600))
    ax1 = Axis(fig[1, 1], xlabel="Return Period (years)", ylabel="Return Level (inches)", title="Prior v1 Predictive Distribution", xscale=log10, xticks=xticks)
    posterior_bands!(ax1, prior_GEVs_v1, rts; color=(:blue, 0.2), ci=0.90)
    posterior_mean_curve!(ax1, prior_GEVs_v1, rts; color=:blue, linewidth=3)
    ax2 = Axis(fig[1, 2], xlabel="Return Period (years)", ylabel="Return Level (inches)", title="Prior v2 Predictive Distribution", xscale=log10, xticks=xticks)
    posterior_bands!(ax2, prior_GEVs_v2, rts; color=(:blue, 0.2), ci=0.90)
    posterior_mean_curve!(ax2, prior_GEVs_v2, rts; color=:blue, linewidth=3)
    linkaxes!(ax1, ax2)
    fig
end
```

::: {.callout-note}
## Think

Do these make sense?
Are the uncertainties too tight or too wide?
Go back and revise your `return_level_priors` if needed.
When you're happy with your answers, set `overwrite=false` so you can load cached results in the future.

Hard to say?  I'm not sure if I really know how tight is too tight, or how wide is too wide.  But it definitely tightened up a lot in V2. 
:::

## Inference 

Now we move from prior-only analysis to combining our prior beliefs with the observed rainfall data!

### Extracting Station Data

First, let's prepare the actual rainfall data for our primary station.
We'll define the model with the return level priors you specified above.
It is very important to drop the missing values from your observed data -- otherwise Turing will treat the missing values as parameters to sample, which is useful for imputation but not what we want here.

```{julia}
#| output: false
y_obs = collect(skipmissing(ustrip.(u"inch", my_rainfall.rainfall)))  # <1>
bayes_model = gev_model_quantile_priors(y_obs; return_level_priors=return_level_priors)
```

1. Remove Unitful units since Turing.jl works with plain numbers, and skip missing values

### Point Estimates: MLE and MAP

Before comparing the full posterior distribution to our prior beliefs, let's also compute point estimates for comparison.
Maximum likelihood estimation (MLE) finds the single parameter values that maximize the likelihood of observing our data.
Maximum a posteriori (MAP) estimation finds the parameter values that maximize the posterior probability (combining likelihood and prior).

We can look at `optim_result` from the MLE optimization to see if it thinks it converged

```{julia}
mle_estimate = maximum_likelihood(bayes_model, NelderMead(); maxiters=1_000)
mle_estimate.optim_result
```

Similarly for MAP

```{julia}
map_estimate = maximum_a_posteriori(bayes_model, NelderMead(); maxiters=1_000)
map_estimate.optim_result
```

We can save the resulting GEV distributions for later plotting

```{julia}
#| output: false
mle_dist = GeneralizedExtremeValue(mle_estimate.values[1], exp(mle_estimate.values[2]), mle_estimate.values[3])
map_dist = GeneralizedExtremeValue(map_estimate.values[1], exp(map_estimate.values[2]), map_estimate.values[3])
```

### Bayesian Posterior Sampling

```{julia}
#| output: false
posterior_idata = let
    fname = joinpath(lab_dir, "posterior_data.nc")
    overwrite = true
    load_or_sample(fname, bayes_model; overwrite=overwrite)
end
posterior_GEVs = vec(GeneralizedExtremeValue.(posterior_idata.posterior.μ, exp.(posterior_idata.posterior.log_σ), posterior_idata.posterior.ξ))
```

Let's compare the prior and posterior distributions visually.

```{julia}
let
    fig = Figure(size=(900, 500))
    rts = logrange(1.1, 250, 500)
    ax1 = Axis(fig[1, 1], xlabel="Return Period (years)", ylabel="Return Level (inches)",
        title="Return Level Uncertainty: Prior vs Posterior", xscale=log10, xticks=[1, 2, 5, 10, 25, 50, 100, 250])
    posterior_bands!(ax1, prior_GEVs_v2, rts; color=(:blue, 0.2), ci=0.90, label="Prior 90% CI")
    posterior_bands!(ax1, posterior_GEVs, rts; color=(:orange, 0.4), ci=0.90, label="Posterior 90% CI")
    posterior_mean_curve!(ax1, posterior_GEVs, rts; color=:blue, linewidth=3, label="Posterior Mean")

    mean_return_levels = [quantile(mle_dist, 1 - 1 / T) for T in rts]
    lines!(ax1, rts, mean_return_levels, color=:red, linewidth=3, label="MLE")

    axislegend(ax1; position=:lt)
    fig
end
```

We can see that both the posterior and the MLE give us similar point estimates, but the Bayesian posterior gives us a full uncertainty distribution, which is very useful for risk assessment.

## Diagnostics

To make sure that our model is reliable, we need to check that our MCMC sampling converged and that our model fits the data well.
We can use automatically generated summary statistics to do this:

```{julia}
ArviZ.summarize(posterior_idata)
```

We can look for:

- $\hat{R}$ values close to 1.0 (indicates chains have converged to same distribution)
- Effective sample sizes (`ess_tail` and `ess_bulk`) that are reasonably large (as close to the total number of samples as possible)

These are not guarantees of convergence, but they indicate the absence of obvious problems.
$\hat{R} > 1.01$ or low effective sample sizes suggest the chains haven't mixed well.

We can also look at trace plots and marginal densities for each parameter.

```{julia}
function plot_trace_diagnostics(idata, title_prefix="")  # <1>
    fig = Figure(size=(1200, 400))

    # Extract chain x draw arrays for each parameter
    μ_arr = Array(idata.posterior.μ)  # <2>
    log_σ_arr = Array(idata.posterior.log_σ)  # <3>
    ξ_arr = Array(idata.posterior.ξ)  # <4>
    ndraws, nchains = size(μ_arr)  # <5>

    # Trace plots per chain
    ax1 = Axis(fig[1, 1], xlabel="Iteration", ylabel="μ",
        title="$title_prefix Location Parameter Trace")
    for c in 1:nchains  # <6>
        lines!(ax1, 1:ndraws, μ_arr[:, c], label="Chain $(c)")  # <7>
    end
    axislegend(ax1; position=:lb)

    ax2 = Axis(fig[1, 2], xlabel="Iteration", ylabel="log(σ)",
        title="$title_prefix Log-Scale Parameter Trace")
    for c in 1:nchains
        lines!(ax2, 1:ndraws, log_σ_arr[:, c])
    end

    ax3 = Axis(fig[1, 3], xlabel="Iteration", ylabel="ξ",
        title="$title_prefix Shape Parameter Trace")
    for c in 1:nchains
        lines!(ax3, 1:ndraws, ξ_arr[:, c])
    end

    return fig
end
fig_trace_primary = plot_trace_diagnostics(posterior_idata, "Primary Station:")
```

1. Function to create trace plots for MCMC diagnostics
2. Extract location parameter samples as a 2D array (draws × chains)
3. Extract log-scale parameter samples
4. Extract shape parameter samples
5. Get dimensions: number of draws per chain and number of chains
6. Loop over each MCMC chain
7. Plot the trace (parameter value vs. iteration) for each chain

```{julia}
function plot_marginal_densities(idata, title_prefix="")
    fig = Figure(size=(1200, 400))

    # Extract chain x draw arrays for each parameter
    μ_arr = Array(idata.posterior.μ)
    log_σ_arr = Array(idata.posterior.log_σ)
    ξ_arr = Array(idata.posterior.ξ)

    # Marginal densities aggregated across chains
    ax1 = Axis(fig[1, 1], xlabel="μ", ylabel="Density",
        title="$title_prefix Location Posterior")
    hist!(ax1, vec(μ_arr), bins=50, normalization=:pdf, color=(:blue, 0.7))

    ax2 = Axis(fig[1, 2], xlabel="log(σ)", ylabel="Density",
        title="$title_prefix Log-Scale Posterior")
    hist!(ax2, vec(log_σ_arr), bins=50, normalization=:pdf, color=(:blue, 0.7))

    ax3 = Axis(fig[1, 3], xlabel="ξ", ylabel="Density",
        title="$title_prefix Shape Posterior")
    hist!(ax3, vec(ξ_arr), bins=50, normalization=:pdf, color=(:blue, 0.7))

    return fig
end

fig_densities_primary = plot_marginal_densities(posterior_idata, "Primary Station:")
```



## Two-Station Comparison

In lab 3, you compared MLE return levels between two stations.
Now we'll do a full Bayesian comparison using the same framework.

### Selecting a Comparison Station

```{julia}
nearest_stations = find_nearest_stations(my_station, stations, 5)
```

We will pick one of these five nearest stations for comparison

```{julia}
comparison_stnid = 93 # MODIFY THIS AND CHOOSE YOUR COMPARISON

comparison_station = @chain stations begin
    @filter(stnid == !!comparison_stnid)
    first
end
comparison_data = @chain rainfall_data begin
    @filter(stnid == !!comparison_stnid)
    @arrange(date)
end

# plot both stations' data for visual comparison
function plot_two_stations(station1, data1, station2, data2)
    fig = Figure(size=(800, 400))
    ax = Axis(fig[1, 1], xlabel="Year", ylabel="Annual Max Rainfall (inches)",
        title="Annual Maximum Rainfall: $(station1.name) vs $(station2.name)")

    years1 = data1.year
    rain1 = ustrip.(u"inch", data1.rainfall)
    lines!(ax, years1, rain1, color=:purple, label=station1.name)
    scatter!(ax, years1, rain1, color=:purple, markersize=8)

    years2 = data2.year
    rain2 = ustrip.(u"inch", data2.rainfall)
    lines!(ax, years2, rain2, color=:orange, label=station2.name)
    scatter!(ax, years2, rain2, color=:orange, markersize=8)

    axislegend(ax; position=:rt)
    return fig
end
plot_two_stations(my_station, my_rainfall, comparison_station, comparison_data)
```

We will use the same model and priors for the comparison station, assuming similar climate.

```{julia}
#| output: false
posterior_idata_comp = let
    fname = joinpath(lab_dir, "posterior_data_comp.nc")
    y_obs_comp = collect(skipmissing(ustrip.(u"inch", comparison_data.rainfall)))
    model = gev_model_quantile_priors(y_obs_comp; return_level_priors=return_level_priors)
    overwrite = true
    load_or_sample(fname, model; overwrite=overwrite)
end
posterior_GEVs_comp = vec(GeneralizedExtremeValue.(posterior_idata_comp.posterior.μ, exp.(posterior_idata_comp.posterior.log_σ), posterior_idata_comp.posterior.ξ))
```

We conduct the same diagnostics for the comparison station

```{julia}
plot_trace_diagnostics(posterior_idata_comp, "Comparison Station:")
```

```{julia}
fig_densities_comparison = plot_marginal_densities(posterior_idata_comp, "Comparison Station:")
```

We can compare the return level uncertainties between the two stations

```{julia}
fig_rl_comp = let
    rts = logrange(1.1, 250, 500)
    xticks = [1, 2, 5, 10, 25, 50, 100, 250]
    fig = Figure(size=(1200, 600))

    ax1 = Axis(fig[1, 1], xlabel="Return Period (years)", ylabel="Return Level (inches)",
        title="$(my_station.name)", xscale=log10, xticks=xticks)
    posterior_bands!(ax1, posterior_GEVs, rts; color=(:blue, 0.2), ci=0.90)
    posterior_mean_curve!(ax1, posterior_GEVs, rts; color=:blue, linewidth=3)

    ax2 = Axis(fig[1, 2], xlabel="Return Period (years)", ylabel="Return Level (inches)",
        title="$(comparison_station.name)", xscale=log10, xticks=xticks)
    posterior_bands!(ax2, posterior_GEVs_comp, rts; color=(:orange, 0.2), ci=0.90)
    posterior_mean_curve!(ax2, posterior_GEVs_comp, rts; color=:orange, linewidth=3)

    linkaxes!(ax1, ax2)
    fig
end
```

::: {.callout-note}
## Think
Compare the posterior distributions between the two stations.
Do the stations show similar extreme rainfall patterns?
Which parameters differ most between stations?
How might geographic or climatic factors explain these differences?
Should we have used different priors for each station?
Do the return level uncertainties overlap significantly, or are they distinct?
What does this tell you about spatial variability in extreme precipitation?

They're fairly similar, but the log-scale posterior is bigger for Galveston.  This could be related to it being coastal, while Alvin is further inland. A coastal location could lend itself to fatter tails with bigger storms, and a larger shape posterior as a result.

Strangely though, this doesn't seem to be reflected on the uncertainty bounds?  Alvin's uncertainty bounds are wider than Galveston's.  It's not reflected in the data either.  The Alvin data has a large spike in it, which could be what the wider boundsa are due to.  So I'm not sure why Alvin has a smaller distribution of the scale posterior. 

Other than that, the stations have fairly simlar parameters.  The return levels do overlap quite significanlty, which makes sense, given the geographic closeness of these two stations.  
:::
